# Prometheus Alert Rules - System Health
# Monitor: CPU, Memory, Disk, Network

groups:
  # =======================================
  # NODE/SYSTEM ALERTS
  # =======================================
  - name: node_health
    interval: 30s
    rules:
      # Instance down
      - alert: InstanceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Instance {{ $labels.instance }} is down"
          description: "{{ $labels.instance }} ({{ $labels.job }}) has been down for more than 2 minutes."

      # High CPU usage
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 85% for 10 minutes on {{ $labels.instance }}."

      # Critical CPU usage
      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 95% for 5 minutes on {{ $labels.instance }}. Immediate action required."

      # High memory usage
      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% on {{ $labels.instance }}."

      # Critical memory usage
      - alert: CriticalMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 95
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Memory exhaustion on {{ $labels.instance }}"
          description: "Memory usage is above 95% on {{ $labels.instance }}. System may OOM soon!"

      # Disk space warning
      - alert: DiskSpaceWarning
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"} * 100) < 20
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Filesystem {{ $labels.mountpoint }} has less than 20% free space on {{ $labels.instance }}."

      # Disk space critical
      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{fstype!~"tmpfs|fuse.lxcfs"} / node_filesystem_size_bytes{fstype!~"tmpfs|fuse.lxcfs"} * 100) < 10
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CRITICAL: Disk almost full on {{ $labels.instance }}"
          description: "Filesystem {{ $labels.mountpoint }} has less than 10% free space on {{ $labels.instance }}!"

      # High disk I/O
      - alert: HighDiskIO
        expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High disk I/O on {{ $labels.instance }}"
          description: "Disk {{ $labels.device }} has sustained high I/O for 10 minutes on {{ $labels.instance }}."

      # System load high
      - alert: HighSystemLoad
        expr: node_load5 / count(node_cpu_seconds_total{mode="idle"}) without(cpu, mode) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High system load on {{ $labels.instance }}"
          description: "System load is more than 2x CPU count on {{ $labels.instance }}."

  # =======================================
  # CONTAINER/DOCKER ALERTS
  # =======================================
  - name: container_health
    interval: 30s
    rules:
      # Container down
      - alert: ContainerDown
        expr: time() - container_last_seen{name!~".*POD.*"} > 60
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} is down"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} has been down for 2 minutes."

      # High container memory
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!~".*POD.*"} / container_spec_memory_limit_bytes{name!~".*POD.*"}) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} high memory"
          description: "Container {{ $labels.name }} is using over 85% of its memory limit on {{ $labels.instance }}."

      # High container CPU
      - alert: ContainerHighCPU
        expr: (rate(container_cpu_usage_seconds_total{name!~".*POD.*"}[5m]) * 100) > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} high CPU"
          description: "Container {{ $labels.name }} is using over 85% CPU on {{ $labels.instance }}."

      # Container restart loop
      - alert: ContainerRestartLoop
        expr: rate(container_restart_count{name!~".*POD.*"}[15m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ $labels.name }} restarting frequently"
          description: "Container {{ $labels.name }} on {{ $labels.instance }} is restarting frequently (crash loop?)."

  # =======================================
  # SERVICE ALERTS
  # =======================================
  - name: service_health
    interval: 30s
    rules:
      # Traefik down
      - alert: TraefikDown
        expr: up{job="traefik-master"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Traefik reverse proxy is DOWN"
          description: "Traefik on master is down. All web services are inaccessible!"

      # Prometheus down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus is DOWN"
          description: "Prometheus monitoring is down. No metrics collection!"

      # Loki down
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Loki log aggregation is DOWN"
          description: "Loki on master is down. No log collection!"

      # Tailscale connectivity
      - alert: TailscaleDisconnected
        expr: up{job=~"node-.*"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Server {{ $labels.instance }} disconnected from Tailnet"
          description: "Cannot reach {{ $labels.instance }} over Tailscale for 5 minutes."
